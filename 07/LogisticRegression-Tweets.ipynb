{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HQ_69fJ8FNrZ"
   },
   "source": [
    "# Aim:\n",
    "* Extract features for logistic regression given some text\n",
    "* Implement logistic regression from scratch\n",
    "* Apply logistic regression on a natural language processing task\n",
    "* Test logistic regression\n",
    "\n",
    "We will be using a data set of tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Bab5_YEFNrf"
   },
   "source": [
    "## Import functions and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ltnC-DbVFNrl"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import twitter_samples \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3805,
     "status": "ok",
     "timestamp": 1599052285796,
     "user": {
      "displayName": "Prof. Hariom Pandya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt3sg6X_951s0boD3SSJvqRng4AQaC3MhTBtGQ9Q=s64",
      "userId": "16159546014304882594"
     },
     "user_tz": -330
    },
    "id": "nEw35lgbIKmJ",
    "outputId": "462924da-f065-40c3-fa7c-52d4163c7aeb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K8zAwNf3IcwC"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VOkRSbEw1Shr"
   },
   "outputs": [],
   "source": [
    "#process_tweet(): cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\n",
    "def process_tweet(tweet):\n",
    "    \"\"\"Process tweet function.\n",
    "    Input:\n",
    "        tweet: a string containing a tweet\n",
    "    Output:\n",
    "        tweets_clean: a list of words containing the processed tweet\n",
    "\n",
    "    \"\"\"\n",
    "    tweet = str(tweet)\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "    # tokenize tweets\n",
    "\n",
    "\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "            \n",
    "            #############################################################\n",
    "            # 1 remove stopwords\n",
    "            # 2 remove punctuation\n",
    "            # 3 stemming word\n",
    "            # 4 Add it to tweets_clean\n",
    "            if(word in stopwords_english):\n",
    "                continue\n",
    "            \n",
    "            punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "            for c in word:\n",
    "                if c in punc:\n",
    "                    word = word.replace(c, \"\")\n",
    "            \n",
    "            word = stemmer.stem(word)\n",
    "            \n",
    "            if len(word) == 0:\n",
    "                continue\n",
    "                \n",
    "            tweets_clean.append(word)\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "f6v_JLxe1ZXh"
   },
   "outputs": [],
   "source": [
    "#build_freqs counts how often a word in the 'corpus' (the entire set of tweets) was associated with\n",
    "  # a positive label '1'         or \n",
    "  # a negative label '0', \n",
    "\n",
    "#then builds the freqs dictionary, where each key is a (word,label) tuple, \n",
    "\n",
    "#and the value is the count of its frequency within the corpus of tweets.\n",
    "\n",
    "def build_freqs(tweets, ys):\n",
    "    \"\"\"Build frequencies.\n",
    "    Input:\n",
    "        tweets: a list of tweets\n",
    "        ys: an m x 1 array with the sentiment label of each tweet\n",
    "            (either 0 or 1)\n",
    "    Output:\n",
    "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
    "        frequency\n",
    "    \"\"\"\n",
    "    # Convert np array to list since zip needs an iterable.\n",
    "    # The squeeze is necessary or the list ends up with one element.\n",
    "    # Also note that this is just a NOP if ys is already a list.\n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "\n",
    "    # Start with an empty dictionary and populate it by looping over all tweets\n",
    "    # and over all processed words in each tweet.\n",
    "    freqs = {}\n",
    "\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            if pair not in freqs:\n",
    "                freqs[pair] = 1\n",
    "            else:\n",
    "                freqs[pair] += 1\n",
    "            \n",
    "            #############################################################\n",
    "            #Update the count of pair if present, set it to 1 otherwise\n",
    "            \n",
    "\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zG07lgkRFNtF"
   },
   "source": [
    "### Prepare the data\n",
    "* The `twitter_samples` contains subsets of 5,000 positive tweets, 5,000 negative tweets, and the full set of 10,000 tweets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mt_btshaFNtJ"
   },
   "outputs": [],
   "source": [
    "# select the set of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bFUFtkzCFNt7"
   },
   "source": [
    "* Train test split: 20% will be in the test set, and 80% in the training set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PNp1zFFmFNuA"
   },
   "outputs": [],
   "source": [
    "# split the data into two pieces, one for training and one for testing\n",
    "#############################################################\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "\n",
    "train_pos, test_pos = train_test_split(all_positive_tweets, test_size = 0.20, random_state = 136) \n",
    "train_neg, test_neg = train_test_split(all_negative_tweets, test_size = 0.20, random_state = 136) \n",
    "\n",
    "X_train = copy.deepcopy(train_pos)\n",
    "X_train.extend(train_neg)\n",
    "\n",
    "X_test=copy.deepcopy(test_pos)\n",
    "X_test.extend(test_neg)\n",
    "\n",
    "train_pos=np.array(train_pos)\n",
    "train_neg=np.array(train_neg)\n",
    "test_pos=np.array(test_pos)\n",
    "test_neg=np.array(test_neg)\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JyC4UAjZFNuT"
   },
   "source": [
    "* Create the numpy array of positive labels and negative labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8000,) (4000,) (4000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,train_pos.shape,train_neg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xS1wvGq7FNuX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine positive and negative labels\n",
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)\n",
    "train_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RvppFUyLFNu2"
   },
   "source": [
    "* Create the frequency dictionary using the  `build_freqs()` function.  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7621,
     "status": "ok",
     "timestamp": 1599052289715,
     "user": {
      "displayName": "Prof. Hariom Pandya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt3sg6X_951s0boD3SSJvqRng4AQaC3MhTBtGQ9Q=s64",
      "userId": "16159546014304882594"
     },
     "user_tz": -330
    },
    "id": "ggrsgUh4FNu5",
    "outputId": "950b9e93-4b1c-4056-c9bc-ebb142247f00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type(freqs) = <class 'dict'>\n",
      "len(freqs) = 11234\n"
     ]
    }
   ],
   "source": [
    "# create frequency dictionary\n",
    "#############################################################\n",
    "freqs = build_freqs(X_train, train_y)\n",
    "\n",
    "# check the output\n",
    "print(\"type(freqs) = \" + str(type(freqs)))\n",
    "print(\"len(freqs) = \" + str(len(freqs.keys())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fJw_OZmF3cK9"
   },
   "source": [
    "* HERE, The `freqs` dictionary is the frequency dictionary that's being built. \n",
    "* The key is the tuple (word, label), such as (\"happy\",1) or (\"happy\",0).  The value stored for each key is the count of how many times the word \"happy\" was associated with a positive label, or how many times \"happy\" was associated with a negative label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MczxmfJiFNvN"
   },
   "source": [
    "### Process tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7603,
     "status": "ok",
     "timestamp": 1599052289718,
     "user": {
      "displayName": "Prof. Hariom Pandya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt3sg6X_951s0boD3SSJvqRng4AQaC3MhTBtGQ9Q=s64",
      "userId": "16159546014304882594"
     },
     "user_tz": -330
    },
    "id": "bixjcwIOFNvP",
    "outputId": "aeff5f44-91af-46eb-c6b8-55fc7f30802e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example of a positive tweet: \n",
      " @EnRoute2BeFit If people don't have anything postive to say they should keep their opinions to themselves :) Looking good / strong as always\n",
      "\n",
      "This is an example of the processed version of the tweet: \n",
      " ['peopl', 'anyth', 'postiv', 'say', 'keep', 'opinion', 'look', 'good', 'strong', 'alway']\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "print('This is an example of a positive tweet: \\n', X_train[0])\n",
    "print('\\nThis is an example of the processed version of the tweet: \\n', process_tweet(X_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cVDYiL7aFNvg"
   },
   "source": [
    "#Logistic regression :\n",
    "\n",
    "\n",
    "### Sigmoid\n",
    "\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}} $$\n",
    "\n",
    "It maps the input 'x' to a value that ranges between 0 and 1, and so it can be treated as a probability. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tlkgwDxIFNvq"
   },
   "outputs": [],
   "source": [
    "def sigmoid(z): \n",
    "       \n",
    "    # calculate the sigmoid of z\n",
    "    #############################################################\n",
    "    h = 1/(1+np.exp(-z))\n",
    "    \n",
    "    return h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EgDmpXRpFNwP"
   },
   "source": [
    "### Logistic regression: regression and a sigmoid\n",
    "\n",
    "Logistic regression takes a regular linear regression, and applies a sigmoid to the output of the linear regression.\n",
    "\n",
    "Logistic regression\n",
    "$$ h(z) = \\frac{1}{1+\\exp^{-z}}$$\n",
    "$$z = \\theta_0 x_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... \\theta_N x_N$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ny2NTg2oFNw6"
   },
   "source": [
    "#### Update the weights:Gradient Descent\n",
    "\n",
    "\n",
    "$$\\nabla_{\\theta_j}J(\\theta) = \\frac{1}{m} \\sum_{i=1}^m(h^{(i)}-y^{(i)})x_j $$\n",
    "\n",
    "* To update the weight $\\theta_j$, we adjust it by subtracting a fraction of the gradient determined by $\\alpha$:\n",
    "$$\\theta_j = \\theta_j - \\alpha \\times \\nabla_{\\theta_j}J(\\theta) $$\n",
    "\n",
    "* The learning rate $\\alpha$ is a value that we choose to control how big a single update will be.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qrFMw4hkFNxA"
   },
   "outputs": [],
   "source": [
    "def gradientDescent(x, y, theta, alpha, num_iters):\n",
    "  \n",
    "    # get 'm', the number of rows in matrix x\n",
    "    m = len(x)\n",
    "    \n",
    "    for i in range(0, num_iters):\n",
    "        # get z, the dot product of x and theta\n",
    "        #############################################################\n",
    "        z =  np.dot(x,theta)\n",
    "        # get the sigmoid of z\n",
    "        #############################################################\n",
    "        h = sigmoid(z)\n",
    "        # calculate the cost function\n",
    "        J = (-1/m)*(y.T @ np.log(h) + (1-y).T @ np.log(1-h))\n",
    "\n",
    "        # update the weights theta\n",
    "        #############################################################\n",
    "        for j in range(len(x[0])):\n",
    "            theta[j][0] = theta[j][0] - alpha * np.sum((h-y)*x[:,j])\n",
    "        \n",
    "    J = float(J)\n",
    "    return J, theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-pMQQEM8FNxr"
   },
   "source": [
    "## Extracting the features\n",
    "\n",
    "* Given a list of tweets, extract the features and store them in a matrix. You will extract two features.\n",
    "    * The first feature is the number of positive words in a tweet.\n",
    "    * The second feature is the number of negative words in a tweet. \n",
    "* Then train your logistic regression classifier on these features.\n",
    "* Test the classifier on a validation set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tBqwBhBoFNx0"
   },
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a list of words for one tweet\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "    Output: \n",
    "        x: a feature vector of dimension (1,3)\n",
    "    '''\n",
    "    # tokenizes, stems, and removes stopwords\n",
    "    #############################################################\n",
    "    word_l = process_tweet(tweet)\n",
    "    \n",
    "    # 3 elements in the form of a 1 x 3 vector\n",
    "    x = np.zeros((1, 3)) \n",
    "    \n",
    "    #bias term is set to 1\n",
    "    x[0,0] = 1 \n",
    "        \n",
    "    # loop through each word in the list of words\n",
    "    for word in word_l:\n",
    "        if (word,1) in freqs:\n",
    "            x[0][1]+=1\n",
    "        if (word,0) in freqs:\n",
    "            x[0][2]+=1\n",
    "            \n",
    "        # increment the word count for the positive label 1\n",
    "        #############################################################\n",
    "        \n",
    "        # increment the word count for the negative label 0\n",
    "        #############################################################\n",
    "    \n",
    "    assert(x.shape == (1, 3))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7572,
     "status": "ok",
     "timestamp": 1599052289731,
     "user": {
      "displayName": "Prof. Hariom Pandya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt3sg6X_951s0boD3SSJvqRng4AQaC3MhTBtGQ9Q=s64",
      "userId": "16159546014304882594"
     },
     "user_tz": -330
    },
    "id": "cElgAfPDFNyC",
    "outputId": "37b193c7-5986-4c41-911c-23f949584e17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1. 10.  8.]]\n"
     ]
    }
   ],
   "source": [
    "# Check the function\n",
    "\n",
    "# test 1\n",
    "# test on training data\n",
    "tmp1 = extract_features(X_train[0], freqs)\n",
    "print(tmp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7554,
     "status": "ok",
     "timestamp": 1599052289733,
     "user": {
      "displayName": "Prof. Hariom Pandya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt3sg6X_951s0boD3SSJvqRng4AQaC3MhTBtGQ9Q=s64",
      "userId": "16159546014304882594"
     },
     "user_tz": -330
    },
    "id": "rvhAbVw2FNyR",
    "outputId": "44251d3f-69a6-4c18-ce62-e74466541ce1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# test 2:\n",
    "# check for when the words are not in the freqs dictionary\n",
    "tmp2 = extract_features('Hariom pandya', freqs)\n",
    "print(tmp2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9oIitRd3FNyi"
   },
   "source": [
    "## Training Your Model\n",
    "\n",
    "To train the model:\n",
    "* Stack the features for all training examples into a matrix `X`. \n",
    "* Call `gradientDescent`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12070,
     "status": "ok",
     "timestamp": 1599052294274,
     "user": {
      "displayName": "Prof. Hariom Pandya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt3sg6X_951s0boD3SSJvqRng4AQaC3MhTBtGQ9Q=s64",
      "userId": "16159546014304882594"
     },
     "user_tz": -330
    },
    "id": "Lgn1YfqAFNyl",
    "outputId": "da2563dc-13dc-45c7-a1a3-0cff8e0158e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "The cost after training is 0.69314718.\n"
     ]
    }
   ],
   "source": [
    "# collect the features 'x' and stack them into a matrix 'X'\n",
    "X = np.zeros((len(X_train), 3))\n",
    "for i in range(len(X_train)):\n",
    "    X[i, :]= extract_features(X_train[i], freqs)\n",
    "\n",
    "# training labels corresponding to X\n",
    "Y = train_y\n",
    "print(len(X[0]))\n",
    "# Apply gradient descent\n",
    "J, theta = gradientDescent(X, Y, np.zeros((3, 1)), 1e-9, 1500)\n",
    "print(f\"The cost after training is {J:.8f}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vll8RGjeFNy5"
   },
   "source": [
    "# Test logistic regression\n",
    "\n",
    "Predict whether a tweet is positive or negative.\n",
    "\n",
    "* Given a tweet, process it, then extract the features.\n",
    "* Apply the model's learned weights on the features to get the logits.\n",
    "* Apply the sigmoid to the logits to get the prediction (a value between 0 and 1).\n",
    "\n",
    "$$y_{pred} = sigmoid(\\mathbf{x} \\cdot \\theta)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EvKNJJecFNy-"
   },
   "outputs": [],
   "source": [
    "def predict_tweet(tweet, freqs, theta):\n",
    "    '''\n",
    "    Input: \n",
    "        tweet: a string\n",
    "        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n",
    "        theta: (3,1) vector of weights\n",
    "    Output: \n",
    "        y_pred: the probability of a tweet being positive or negative\n",
    "    '''\n",
    "    \n",
    "    # extract the features of the tweet and store it into x\n",
    "    #############################################################\n",
    "    x = extract_features(tweet,freqs)\n",
    "    \n",
    "    # make the prediction using x and theta\n",
    "    #############################################################\n",
    "    y_pred = sigmoid(np.dot(x,theta))\n",
    "    \n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12059,
     "status": "ok",
     "timestamp": 1599052294288,
     "user": {
      "displayName": "Prof. Hariom Pandya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt3sg6X_951s0boD3SSJvqRng4AQaC3MhTBtGQ9Q=s64",
      "userId": "16159546014304882594"
     },
     "user_tz": -330
    },
    "id": "iPWmGHacFNzK",
    "outputId": "d721e1a2-cb10-45eb-b1c8-f5e729869cc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am happy -> 0.500000\n",
      "I am bad -> 0.500000\n",
      "this movie should have been great. -> 0.500000\n",
      "great -> 0.500000\n",
      "great great -> 0.500000\n",
      "great great great -> 0.500000\n",
      "great great great great -> 0.500000\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to test your function\n",
    "for tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n",
    "    print( '%s -> %f' % (tweet, predict_tweet(tweet, freqs, theta)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pbVn_RgtFNzr"
   },
   "source": [
    "## Check performance using the test set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5g1-WUwMFNz1"
   },
   "outputs": [],
   "source": [
    "def test_logistic_regression(test_x, test_y, freqs, theta):\n",
    "    \"\"\"\n",
    "    Input: \n",
    "        test_x: a list of tweets\n",
    "        test_y: (m, 1) vector with the corresponding labels for the list of tweets\n",
    "        freqs: a dictionary with the frequency of each pair (or tuple)\n",
    "        theta: weight vector of dimension (3, 1)\n",
    "    Output: \n",
    "        accuracy: (# of tweets classified correctly) / (total # of tweets)\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # the list for storing predictions\n",
    "    y_hat = []\n",
    "    \n",
    "    for tweet in test_x:\n",
    "        # get the label prediction for the tweet\n",
    "        y_pred = predict_tweet(tweet, freqs, theta)\n",
    "        \n",
    "        if y_pred > 0.5:\n",
    "            # append 1.0 to the list\n",
    "            y_hat.append(1)\n",
    "        else:\n",
    "            # append 0 to the list\n",
    "            y_hat.append(0)\n",
    "\n",
    "    # With the above implementation, y_hat is a list, but test_y is (m,1) array\n",
    "    # convert both to one-dimensional arrays in order to compare them using the '==' operator\n",
    "    count=0\n",
    "    y_hat=np.array(y_hat)\n",
    "    m=len(test_y)\n",
    "    #print(m)\n",
    "    \n",
    "    test_y=np.reshape(test_y,m)\n",
    "    #print(y_hat.shape)\n",
    "    #print(test_y.shape)\n",
    "    \n",
    "    accuracy = ((test_y == y_hat).sum())/m\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12517,
     "status": "ok",
     "timestamp": 1599052294768,
     "user": {
      "displayName": "Prof. Hariom Pandya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt3sg6X_951s0boD3SSJvqRng4AQaC3MhTBtGQ9Q=s64",
      "userId": "16159546014304882594"
     },
     "user_tz": -330
    },
    "id": "LxAc-fnuFN0E",
    "outputId": "73e36763-b653-4ec0-e16b-97c732546551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression model's accuracy = 0.5000\n"
     ]
    }
   ],
   "source": [
    "tmp_accuracy = test_logistic_regression(X_test, test_y, freqs, theta)\n",
    "print(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7dDZTqdd_ggw"
   },
   "source": [
    "# Lab Assignment:\n",
    "\n",
    "## Replace Manual version of Logistic Regression with TF based version. \n",
    "#### [Reference : Lab-6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import twitter_samples \n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\harsh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n",
    "\n",
    "train_pos, test_pos = train_test_split(all_positive_tweets, test_size = 0.20, random_state = 136) \n",
    "train_neg, test_neg = train_test_split(all_negative_tweets, test_size = 0.20, random_state = 136) \n",
    "\n",
    "X_train = copy.deepcopy(train_pos)\n",
    "X_train.extend(train_neg)\n",
    "\n",
    "X_test=copy.deepcopy(test_pos)\n",
    "X_test.extend(test_neg)\n",
    "\n",
    "train_pos=np.array(train_pos)\n",
    "train_neg=np.array(train_neg)\n",
    "test_pos=np.array(test_pos)\n",
    "test_neg=np.array(test_neg)\n",
    "\n",
    "train_x = np.array(X_train)\n",
    "test_x = np.array(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweet(tweet):\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    "   \n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n",
    "                               reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    "\n",
    "    tweets_clean = []\n",
    "    for word in tweet_tokens:\n",
    "            if(word not in stopwords_english and word not in string.punctuation):\n",
    "                steam_word = stemmer.stem(word)\n",
    "                tweets_clean.append(steam_word)\n",
    "    return tweets_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_freqs(tweets, ys):\n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "    freqs = {}\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1\n",
    "    return freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       ...,\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\n",
    "test_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)\n",
    "train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "type(freqs) = <class 'dict'>\n",
      "len(freqs) = 11419\n"
     ]
    }
   ],
   "source": [
    "freqs = build_freqs(train_x,train_y)\n",
    "\n",
    "print(\"\\ntype(freqs) = \" + str(type(freqs)))\n",
    "print(\"len(freqs) = \" + str(len(freqs.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of positive tweet: \n",
      " @EnRoute2BeFit If people don't have anything postive to say they should keep their opinions to themselves :) Looking good / strong as always\n",
      "\n",
      "Example of the processed version of the tweet: \n",
      " ['peopl', 'anyth', 'postiv', 'say', 'keep', 'opinion', ':)', 'look', 'good', 'strong', 'alway']\n"
     ]
    }
   ],
   "source": [
    "print('Example of positive tweet: \\n', train_x[0])\n",
    "print('\\nExample of the processed version of the tweet: \\n', process_tweet(train_x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(tweet, freqs):\n",
    " \n",
    "    word_l = process_tweet(tweet)\n",
    "    x = np.zeros((1, 2)) \n",
    "    \n",
    "    for word in word_l:\n",
    "        if((word,1) in freqs):\n",
    "            x[0,0]+=freqs[word,1]\n",
    "    \n",
    "        if((word,0) in freqs):\n",
    "            x[0,1]+=freqs[word,0]\n",
    "    \n",
    "    assert(x.shape == (1, 2))\n",
    "    return x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3387.  352.]\n"
     ]
    }
   ],
   "source": [
    "tmp1 = extract_features(train_x[0], freqs)\n",
    "print(tmp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tweet(tweet):\n",
    "    with tf.Session() as sess:\n",
    "        saver.restore(sess,save_path='TSession')\n",
    "        data_i=[]\n",
    "        for t in tweet:\n",
    "            data_i.append(extract_features(t,freqs))\n",
    "        data_i=np.asarray(data_i)\n",
    "        return sess.run(tf.nn.sigmoid(tf.add(tf.matmul(a=data_i,b=W,transpose_b=True),b)))\n",
    "        print(\"Fail\")\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=tf.Variable(np.random.randn(1),name=\"Bias\")\n",
    "W=tf.Variable(np.random.randn(1,2),name=\"Bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "for t in train_x:\n",
    "    data.append(extract_features(t,freqs))\n",
    "data=np.asarray(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Sigmoid:0\", shape=(8000, 1), dtype=float64)\n",
      "\n",
      " Tensor(\"logistic_loss:0\", shape=(8000, 1), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "Y_hat = tf.nn.sigmoid(tf.add(tf.matmul(np.asarray(data), W,transpose_b=True), b)) \n",
    "print(Y_hat)\n",
    "ta=np.asarray(train_y)\n",
    "cost = tf.nn.sigmoid_cross_entropy_with_logits( \n",
    "                    logits = Y_hat, labels = ta) \n",
    "print(\"\\n\",cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1e-4,name=\"GradientDescent\").minimize(cost) \n",
    "init = tf.global_variables_initializer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bias [0.28828369]\n",
      "Weight [[ 0.78697415 -0.13872724]]\n",
      "Accuracy 0.92975\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "  \n",
    "    sess.run(init)\n",
    "    print(\"Bias\",sess.run(b))\n",
    "    print(\"Weight\",sess.run(W))\n",
    "    for epoch in range(400):\n",
    "        sess.run(optimizer)\n",
    "        preds=sess.run(Y_hat)\n",
    "        acc=((preds==ta).sum())/len(train_y)\n",
    "        accu=[]\n",
    "        repoch=False\n",
    "        if repoch:\n",
    "            accu.append(acc)\n",
    "        if epoch % 1000 == 0:\n",
    "            print(\"Accuracy\",acc)\n",
    "        saved_path = saver.save(sess, 'TSession')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from TSession\n",
      "[[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]] 2000\n"
     ]
    }
   ],
   "source": [
    "preds=predict_tweet(test_x)\n",
    "print(preds,len(test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(x,y):\n",
    "    if len(x)!=len(y):\n",
    "        return\n",
    "    return ((x==y).sum())/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9215"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_accuracy(preds,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "LogisticRegression-Tweets.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/pandyahariom/ML_Lab/blob/master/L6-LogisticRegression/tweeterAnalysis.ipynb",
     "timestamp": 1596958929064
    }
   ]
  },
  "coursera": {
   "schema_names": [
    "NLPC1-1"
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
