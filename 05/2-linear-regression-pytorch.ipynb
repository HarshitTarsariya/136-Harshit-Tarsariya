{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1604,
     "status": "ok",
     "timestamp": 1596557525423,
     "user": {
      "displayName": "Prof. Hariom Pandya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt3sg6X_951s0boD3SSJvqRng4AQaC3MhTBtGQ9Q=s64",
      "userId": "16159546014304882594"
     },
     "user_tz": -330
    },
    "id": "v0BtAX1--7l_"
   },
   "outputs": [],
   "source": [
    "# Import Numpy & PyTorch\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ecc6e79cdfb6a8ca882895ccc895b61b960b0a04",
    "colab_type": "text",
    "id": "i1HSrBDb-7t9"
   },
   "source": [
    "## Linear Regression Model using PyTorch built-ins\n",
    "\n",
    "Let's re-implement the same model using some built-in functions and classes from PyTorch.\n",
    "\n",
    "And now using two different targets: Apples and Oranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "_uuid": "ce66cf0d09a3f38bf2f00ea40418c56d98f1f814",
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2201,
     "status": "ok",
     "timestamp": 1596557526042,
     "user": {
      "displayName": "Prof. Hariom Pandya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt3sg6X_951s0boD3SSJvqRng4AQaC3MhTBtGQ9Q=s64",
      "userId": "16159546014304882594"
     },
     "user_tz": -330
    },
    "id": "iXiEK54j-7t-"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "_uuid": "74bb18bd01ac809079eeb8d05695206e8ba02069",
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2186,
     "status": "ok",
     "timestamp": 1596557526045,
     "user": {
      "displayName": "Prof. Hariom Pandya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt3sg6X_951s0boD3SSJvqRng4AQaC3MhTBtGQ9Q=s64",
      "userId": "16159546014304882594"
     },
     "user_tz": -330
    },
    "id": "wCsxgTWO-7uM"
   },
   "outputs": [],
   "source": [
    "# Input (temp, rainfall, humidity)\n",
    "inputs = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], [102, 43, 37], [69, 96, 70], [73, 67, 43], [91, 88, 64], [87, 134, 58], [102, 43, 37], [69, 96, 70], [73, 67, 43], [91, 88, 64], [87, 134, 58], [102, 43, 37], [69, 96, 70]], dtype='float32')\n",
    "# Targets (apples, oranges)\n",
    "targets = np.array([[56, 70], [81, 101], [119, 133], [22, 37], [103, 119], \n",
    "                    [56, 70], [81, 101], [119, 133], [22, 37], [103, 119], \n",
    "                    [56, 70], [81, 101], [119, 133], [22, 37], [103, 119]], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "_uuid": "d94b355f55250e9c7dcff668920f02d7c5c04925",
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2169,
     "status": "ok",
     "timestamp": 1596557526049,
     "user": {
      "displayName": "Prof. Hariom Pandya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt3sg6X_951s0boD3SSJvqRng4AQaC3MhTBtGQ9Q=s64",
      "userId": "16159546014304882594"
     },
     "user_tz": -330
    },
    "id": "nJRlm4-N-7uY"
   },
   "outputs": [],
   "source": [
    "inputs = torch.from_numpy(inputs)\n",
    "targets = torch.from_numpy(targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a0665466eb5401f40a816b323a34450b2c052c41",
    "colab_type": "text",
    "id": "O6JT5Ng6-7uj"
   },
   "source": [
    "### Dataset and DataLoader\n",
    "\n",
    "We'll create a `TensorDataset`, which allows access to rows from `inputs` and `targets` as tuples. We'll also create a DataLoader, to split the data into batches while training. It also provides other utilities like shuffling and sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "_uuid": "206f5fd0473386476b23477bf38d2c327b6376c9",
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2156,
     "status": "ok",
     "timestamp": 1596557526052,
     "user": {
      "displayName": "Prof. Hariom Pandya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt3sg6X_951s0boD3SSJvqRng4AQaC3MhTBtGQ9Q=s64",
      "userId": "16159546014304882594"
     },
     "user_tz": -330
    },
    "id": "iGYdbuWc-7ul"
   },
   "outputs": [],
   "source": [
    "# Import tensor dataset & data loader\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "_uuid": "c47a4f2f86fda3918094e01cf7ab0698bbb5acc7",
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2147,
     "status": "ok",
     "timestamp": 1596557526056,
     "user": {
      "displayName": "Prof. Hariom Pandya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt3sg6X_951s0boD3SSJvqRng4AQaC3MhTBtGQ9Q=s64",
      "userId": "16159546014304882594"
     },
     "user_tz": -330
    },
    "id": "LY_cq6Bf-7ux"
   },
   "outputs": [],
   "source": [
    "# Define dataset\n",
    "dataset = torch.utils.data.TensorDataset(inputs,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "_uuid": "0a2f69126319d738b82ae67d5d404ecd6161bfac",
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2139,
     "status": "ok",
     "timestamp": 1596557526059,
     "user": {
      "displayName": "Prof. Hariom Pandya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt3sg6X_951s0boD3SSJvqRng4AQaC3MhTBtGQ9Q=s64",
      "userId": "16159546014304882594"
     },
     "user_tz": -330
    },
    "id": "I-_dMpco-7u-"
   },
   "outputs": [],
   "source": [
    "# Define data loader\n",
    "dataloader = torch.utils.data.DataLoader(dataset,batch_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "276a262e1b9e3a048bcd32989013f9c501c59037",
    "colab_type": "text",
    "id": "Dq8gUbVx-7vK"
   },
   "source": [
    "### nn.Linear\n",
    "Instead of initializing the weights & biases manually, we can define the model using `nn.Linear`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "_uuid": "59da3506559a0640d80d18f77b02726a1757be2f",
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2128,
     "status": "ok",
     "timestamp": 1596557526062,
     "user": {
      "displayName": "Prof. Hariom Pandya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt3sg6X_951s0boD3SSJvqRng4AQaC3MhTBtGQ9Q=s64",
      "userId": "16159546014304882594"
     },
     "user_tz": -330
    },
    "id": "sKa873ZD-7vN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x00000209C64EF820>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define model\n",
    "model = nn.Linear(inputs.shape[1],targets.shape[1])\n",
    "model.parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b3a4a8c499a4680f2533329712de034671dd1cdd",
    "colab_type": "text",
    "id": "rku14lz3-7vX"
   },
   "source": [
    "### Optimizer\n",
    "Instead of manually manipulating the weights & biases using gradients, we can use the optimizer `optim.SGD`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "_uuid": "1848398bd1ced8c25a7bb55612cf32a774500280",
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2117,
     "status": "ok",
     "timestamp": 1596557526064,
     "user": {
      "displayName": "Prof. Hariom Pandya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt3sg6X_951s0boD3SSJvqRng4AQaC3MhTBtGQ9Q=s64",
      "userId": "16159546014304882594"
     },
     "user_tz": -330
    },
    "id": "Yd4H-T8g-7va"
   },
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "28cbe62be55010bd11b31d819cff38da5a772b18",
    "colab_type": "text",
    "id": "V2ktEA-C-7vl"
   },
   "source": [
    "### Loss Function\n",
    "Instead of defining a loss function manually, we can use the built-in loss function `mse_loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "_uuid": "69d7f4e8e27ccd077f711da27f8bede8aa711893",
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2110,
     "status": "ok",
     "timestamp": 1596557526068,
     "user": {
      "displayName": "Prof. Hariom Pandya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt3sg6X_951s0boD3SSJvqRng4AQaC3MhTBtGQ9Q=s64",
      "userId": "16159546014304882594"
     },
     "user_tz": -330
    },
    "id": "TF2xmzgO-7vo"
   },
   "outputs": [],
   "source": [
    "# Import nn.functional\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "_uuid": "a02ff888ed4be720fd9ca376022d8fdcf2559683",
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2099,
     "status": "ok",
     "timestamp": 1596557526070,
     "user": {
      "displayName": "Prof. Hariom Pandya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt3sg6X_951s0boD3SSJvqRng4AQaC3MhTBtGQ9Q=s64",
      "userId": "16159546014304882594"
     },
     "user_tz": -330
    },
    "id": "hSgxvr8N-7vz"
   },
   "outputs": [],
   "source": [
    "# Define loss function\n",
    "loss_fn = F.mse_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e833614a69ff18c554a3d89f643ae2f11e0260f6",
    "colab_type": "text",
    "id": "9jbPdkiO-7wM"
   },
   "source": [
    "### Train the model\n",
    "\n",
    "We are ready to train the model now. We can define a utility function `fit` which trains the model for a given number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "_uuid": "128bc7260221f5338edf8b503c75f0c7d1cce7e8",
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2081,
     "status": "ok",
     "timestamp": 1596557526075,
     "user": {
      "displayName": "Prof. Hariom Pandya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt3sg6X_951s0boD3SSJvqRng4AQaC3MhTBtGQ9Q=s64",
      "userId": "16159546014304882594"
     },
     "user_tz": -330
    },
    "id": "zDnWui7g-7wP"
   },
   "outputs": [],
   "source": [
    "# Define a utility function to train the model\n",
    "def fit(num_epochs, model, loss_fn, opt):\n",
    "    for epoch in range(num_epochs):\n",
    "        for xb,yb in dataloader:\n",
    "#             Generate predictions\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(yb,pred)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        print('Training loss: ', loss_fn(model(inputs), targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "_uuid": "ae8ca4686cf6a68f6c9ca93bf3d227abe96c2201",
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2073,
     "status": "ok",
     "timestamp": 1596557526078,
     "user": {
      "displayName": "Prof. Hariom Pandya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt3sg6X_951s0boD3SSJvqRng4AQaC3MhTBtGQ9Q=s64",
      "userId": "16159546014304882594"
     },
     "user_tz": -330
    },
    "id": "Gd8tiT_q-7wa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:  tensor(3287.7698, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(966.5102, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(345.9995, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(158.8325, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(89.2005, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(56.0684, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(37.0885, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(25.0959, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(17.1931, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(11.8986, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(8.3281, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(5.9131, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(4.2764, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(3.1651, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(2.4089, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(1.8927, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(1.5389, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(1.2950, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(1.1257, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(1.0069, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.9226, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.8617, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.8168, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.7830, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.7568, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.7360, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.7190, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.7047, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.6923, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.6815, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.6718, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.6630, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.6550, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.6475, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.6406, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.6341, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.6281, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.6224, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.6170, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.6120, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.6072, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.6027, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5985, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5945, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5907, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5871, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5837, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5805, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5775, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5746, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5719, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5694, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5670, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5647, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5626, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5605, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5586, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5568, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5550, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5534, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5519, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5504, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5491, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5478, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5465, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5454, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5443, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5432, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5423, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5413, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5405, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5396, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5388, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5381, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5374, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5367, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5361, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5355, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5350, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5345, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5339, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5335, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5330, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5326, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5322, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5318, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5315, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5311, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5308, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5305, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5302, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5300, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5297, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5295, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5293, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5290, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5288, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5286, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5285, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5283, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5281, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5280, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5278, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5277, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5276, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5274, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5273, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5272, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5271, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5270, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5269, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5268, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5268, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5267, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5266, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5265, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5265, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5264, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5263, grad_fn=<MseLossBackward>)\n",
      "Training loss:  tensor(0.5263, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Train the model for 100 epochs\n",
    "fit(120 , model , loss_fn, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "_uuid": "32588a47d0478772a1f08fa55874a322630bd0b6",
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2065,
     "status": "ok",
     "timestamp": 1596557526080,
     "user": {
      "displayName": "Prof. Hariom Pandya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt3sg6X_951s0boD3SSJvqRng4AQaC3MhTBtGQ9Q=s64",
      "userId": "16159546014304882594"
     },
     "user_tz": -330
    },
    "id": "c3Bf-Emn-7wj"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 57.2551,  70.3341],\n",
       "        [ 82.1251, 100.6674],\n",
       "        [118.7363, 132.9251],\n",
       "        [ 21.0867,  37.0038],\n",
       "        [101.8771, 119.1711],\n",
       "        [ 57.2551,  70.3341],\n",
       "        [ 82.1251, 100.6674],\n",
       "        [118.7363, 132.9251],\n",
       "        [ 21.0867,  37.0038],\n",
       "        [101.8771, 119.1711],\n",
       "        [ 57.2551,  70.3341],\n",
       "        [ 82.1251, 100.6674],\n",
       "        [118.7363, 132.9251],\n",
       "        [ 21.0867,  37.0038],\n",
       "        [101.8771, 119.1711]], grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate predictions\n",
    "preds = model(inputs)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "_uuid": "12d757c0f37c2e3af65cf9d4b59878cc10c65acf",
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2659,
     "status": "ok",
     "timestamp": 1596557526686,
     "user": {
      "displayName": "Prof. Hariom Pandya",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Ggt3sg6X_951s0boD3SSJvqRng4AQaC3MhTBtGQ9Q=s64",
      "userId": "16159546014304882594"
     },
     "user_tz": -330
    },
    "id": "_gDGmiHl-7wr"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.],\n",
       "        [ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.],\n",
       "        [ 56.,  70.],\n",
       "        [ 81., 101.],\n",
       "        [119., 133.],\n",
       "        [ 22.,  37.],\n",
       "        [103., 119.]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare with targets\n",
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2448d9832722f4f2813f8bd80b91daefd901dc2e",
    "colab_type": "text",
    "id": "b9nvUidI-7xD"
   },
   "source": [
    "Now we can define the model, optimizer and loss function exactly as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eAyCq48TMlWJ"
   },
   "source": [
    "#Exercise 1:\n",
    " Try Linear Regression just using numpy (Without Tensorflow/Pytorch or other torch library). You can optionally use sklearn (if you want)\n",
    "#Exercise 2:\n",
    " Try Linear regression on same prediction data using Tensorflow\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,Y_train,Y_test = train_test_split(inputs,targets,test_size=0.3,random_state=136)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 20.568804  37.100315]\n",
      " [ 20.568804  37.100315]\n",
      " [ 57.442204  69.89892 ]\n",
      " [ 82.11636  100.92175 ]\n",
      " [ 57.442204  69.89892 ]]\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 22.,  37.],\n",
      "        [ 22.,  37.],\n",
      "        [ 56.,  70.],\n",
      "        [ 81., 101.],\n",
      "        [ 56.,  70.]])\n"
     ]
    }
   ],
   "source": [
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9549499\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(mean_squared_error(Y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.array([[73, 67, 43], [91, 88, 64], [87, 134, 58], [102, 43, 37], [69, 96, 70],[73, 67, 43], [91, 88, 64], \n",
    "                   [87, 134, 58], [102, 43, 37], [69, 96, 70], [73, 67, 43], [91, 88, 64], [87, 134, 58], [102, 43, 37], \n",
    "                   [69, 96, 70]], dtype='float64')\n",
    "\n",
    "targets = np.array([[56, 70], [81, 101], [119, 133], [22, 37], [103, 119],[56, 70], [81, 101], [119, 133], [22, 37], \n",
    "                    [103, 119],[56, 70], [81, 101], [119, 133], [22, 37], [103, 119]], dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets :\n",
      " <tf.Variable 'Variable:0' shape=(15, 2) dtype=float64, numpy=\n",
      "array([[ 56.,  70.],\n",
      "       [ 81., 101.],\n",
      "       [119., 133.],\n",
      "       [ 22.,  37.],\n",
      "       [103., 119.],\n",
      "       [ 56.,  70.],\n",
      "       [ 81., 101.],\n",
      "       [119., 133.],\n",
      "       [ 22.,  37.],\n",
      "       [103., 119.],\n",
      "       [ 56.,  70.],\n",
      "       [ 81., 101.],\n",
      "       [119., 133.],\n",
      "       [ 22.,  37.],\n",
      "       [103., 119.]])>\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.Variable(inputs)\n",
    "targets = tf.Variable(targets)\n",
    "print(\"targets :\\n\",targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(3, 2) dtype=float64, numpy=\n",
      "array([[0.54638447, 0.64195077],\n",
      "       [0.84526841, 0.87724339],\n",
      "       [0.08659259, 0.63145962]])>\n",
      "\n",
      "<tf.Variable 'Variable:0' shape=(2,) dtype=float64, numpy=array([-0.57545064, -1.34702049])>\n"
     ]
    }
   ],
   "source": [
    "v = np.random.rand(3,2)\n",
    "r = np.random.randn(2)\n",
    "v = tf.Variable(v)\n",
    "r = tf.Variable(r)\n",
    "\n",
    "print(v)\n",
    "print()\n",
    "print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(s):\n",
    "    return s @ v + r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(15, 2), dtype=float64, numpy=\n",
       "array([[ 99.66708053, 131.44345715],\n",
       "       [129.07108201, 174.68133445],\n",
       "       [165.24833544, 208.67796976],\n",
       "       [ 94.70623275, 125.21743033],\n",
       "       [124.33232652, 171.36512237],\n",
       "       [ 99.66708053, 131.44345715],\n",
       "       [129.07108201, 174.68133445],\n",
       "       [165.24833544, 208.67796976],\n",
       "       [ 94.70623275, 125.21743033],\n",
       "       [124.33232652, 171.36512237],\n",
       "       [ 99.66708053, 131.44345715],\n",
       "       [129.07108201, 174.68133445],\n",
       "       [165.24833544, 208.67796976],\n",
       "       [ 94.70623275, 125.21743033],\n",
       "       [124.33232652, 171.36512237]])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model(inputs)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(3755.362945061839, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "def mse(t1,t2):\n",
    "    return tf.reduce_mean(tf.square(t1 - t2))\n",
    "print(mse(prediction,targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \n",
      " Loss: 20.106306785888883 \n",
      "\n",
      "\n",
      "Epoch: 1 \n",
      " Loss: 19.5618259073313 \n",
      "\n",
      "\n",
      "Epoch: 2 \n",
      " Loss: 19.049542248351578 \n",
      "\n",
      "\n",
      "Epoch: 3 \n",
      " Loss: 18.566148318810846 \n",
      "\n",
      "\n",
      "Epoch: 4 \n",
      " Loss: 18.108730714662446 \n",
      "\n",
      "\n",
      "Epoch: 5 \n",
      " Loss: 17.67472188014168 \n",
      "\n",
      "\n",
      "Epoch: 6 \n",
      " Loss: 17.261857827573557 \n",
      "\n",
      "\n",
      "Epoch: 7 \n",
      " Loss: 16.86814106736379 \n",
      "\n",
      "\n",
      "Epoch: 8 \n",
      " Loss: 16.491808098681084 \n",
      "\n",
      "\n",
      "Epoch: 9 \n",
      " Loss: 16.131300894924063 \n",
      "\n",
      "\n",
      "Epoch: 10 \n",
      " Loss: 15.785241889932232 \n",
      "\n",
      "\n",
      "Epoch: 11 \n",
      " Loss: 15.452412033032045 \n",
      "\n",
      "\n",
      "Epoch: 12 \n",
      " Loss: 15.13173153494807 \n",
      "\n",
      "\n",
      "Epoch: 13 \n",
      " Loss: 14.822242973571406 \n",
      "\n",
      "\n",
      "Epoch: 14 \n",
      " Loss: 14.523096469552165 \n",
      "\n",
      "\n",
      "Epoch: 15 \n",
      " Loss: 14.233536677497908 \n",
      "\n",
      "\n",
      "Epoch: 16 \n",
      " Loss: 13.952891369884112 \n",
      "\n",
      "\n",
      "Epoch: 17 \n",
      " Loss: 13.68056141821786 \n",
      "\n",
      "\n",
      "Epoch: 18 \n",
      " Loss: 13.416012000027585 \n",
      "\n",
      "\n",
      "Epoch: 19 \n",
      " Loss: 13.158764881313486 \n",
      "\n",
      "\n",
      "Epoch: 20 \n",
      " Loss: 12.908391642560854 \n",
      "\n",
      "\n",
      "Epoch: 21 \n",
      " Loss: 12.664507732608877 \n",
      "\n",
      "\n",
      "Epoch: 22 \n",
      " Loss: 12.426767248869863 \n",
      "\n",
      "\n",
      "Epoch: 23 \n",
      " Loss: 12.194858354848744 \n",
      "\n",
      "\n",
      "Epoch: 24 \n",
      " Loss: 11.96849925683856 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "for epoch_count in range(epochs):\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as t:\n",
    "        current_loss = mse(targets, model(inputs))\n",
    "\n",
    "    v1 = t.gradient(current_loss,v)\n",
    "    r1 = t.gradient(current_loss,r)\n",
    "\n",
    "    v.assign_sub(1e-4 * v1)\n",
    "    r.assign_sub(1e-4 * r1)\n",
    "        \n",
    "    print(f\"Epoch: {epoch_count} \\n Loss: {current_loss.numpy()} \\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(15, 2), dtype=float64, numpy=\n",
       "array([[ 57.48549059,  70.37593437],\n",
       "       [ 79.53981645,  99.03684611],\n",
       "       [124.24196763, 136.56129279],\n",
       "       [ 23.21788831,  38.55120985],\n",
       "       [ 95.91925763, 115.13311738],\n",
       "       [ 57.48549059,  70.37593437],\n",
       "       [ 79.53981645,  99.03684611],\n",
       "       [124.24196763, 136.56129279],\n",
       "       [ 23.21788831,  38.55120985],\n",
       "       [ 95.91925763, 115.13311738],\n",
       "       [ 57.48549059,  70.37593437],\n",
       "       [ 79.53981645,  99.03684611],\n",
       "       [124.24196763, 136.56129279],\n",
       "       [ 23.21788831,  38.55120985],\n",
       "       [ 95.91925763, 115.13311738]])>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "2-linear-regression-pytorch.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
